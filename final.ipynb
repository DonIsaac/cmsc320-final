{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient, UpdateOne # https://pymongo.readthedocs.io/en/stable/tutorial.html\n",
    "import dns\n",
    "import requests\n",
    "import requests_cache # https://requests-cache.readthedocs.io/en/stable/\n",
    "import datetime\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import json\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to MongoDB\n",
    "_mongo_uri = os.getenv('MONGO_URI')\n",
    "# _mongo_uri = 'mongodb+srv://admin:<password>@cmsc320-final-tutorial.i5dh9.mongodb.net/myFirstDatabase?retryWrites=true&w=majority'\n",
    "# print(_mongo_uri)\n",
    "if not _mongo_uri:\n",
    "    raise Exception('MONGO_URI not set')\n",
    "\n",
    "client = MongoClient(_mongo_uri, tls=True)\n",
    "db = client['stackOverflowDB']\n",
    "questions = db['questions']\n",
    "answers = db['answers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up request caching for StackOverflow API\n",
    "session = requests_cache.CachedSession('.cache/stack_cache', cache_control=True, stale_if_error=True, backend='filesystem')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting StackOverflow Questions\n",
    "\n",
    "Questions are procured from the [StackOverflow REST API](https://api.stackexchange.com/docs), specifically the [/questions endpoint](https://api.stackexchange.com/docs/questions#order=desc&sort=activity&tagged=c%3Bc%2B%2B&filter=default&site=stackoverflow). We'll be limiting our search to C/C++ code snippets for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stackoverflow_questions(**kwargs):\n",
    "    \n",
    "    pagesize = kwargs.get('pagesize', 100) # How many questions to return per page\n",
    "    assert 1 <= pagesize <= 100            # Stack allows [0, 100] but why waste API calls?\n",
    "\n",
    "    page = kwargs.get('page', 1)           # Starting page index, 1-indexed\n",
    "    assert page >= 1                       \n",
    "\n",
    "    maxpages = kwargs.get('maxpages', 10)  # Max number of pages to return\n",
    "    assert maxpages >= 1\n",
    "\n",
    "    question_boundary_younger = datetime.datetime(2021, 12, 4) # No questions posted more recently than this will be returned\n",
    "    done = False # Set to True if we hit our request quota or no more question data is available\n",
    "    requests_made = 0\n",
    "\n",
    "    while not done and requests_made < maxpages:\n",
    "        query_params = {\n",
    "            'site': 'stackoverflow',\n",
    "            'sort': 'activity',\n",
    "            'order': 'desc',\n",
    "            'tagged': 'c',\n",
    "            'page': page,\n",
    "            'pagesize': pagesize,\n",
    "            'todate': int(question_boundary_younger.timestamp())\n",
    "        }\n",
    "\n",
    "        # Returns a Common Wrapper Object\n",
    "        # https://api.stackexchange.com/docs/wrapper\n",
    "        print(f'Fetching page {page}')\n",
    "        r = session.get('https://api.stackexchange.com/2.3/questions', params=query_params)\n",
    "        if r.status_code > 299:\n",
    "            if r.headers['content-length'] == 0:\n",
    "                r.raise_for_status()\n",
    "            else:\n",
    "                error_json = r.json()\n",
    "                raise requests.HTTPError(f'{r.status_code} {r.reason} API returned error {error_json[\"error_id\"]}: {error_json[\"error_message\"]}')\n",
    "                \n",
    "        assert 'json' in r.headers['content-type'] # We're expecting JSON back\n",
    "\n",
    "        requests_made += 1\n",
    "        page += 1\n",
    "\n",
    "        # Yield each question in the response\n",
    "        body = r.json()\n",
    "        assert 'items' in body\n",
    "        assert isinstance(body['items'], list)\n",
    "        yield body['items']\n",
    "\n",
    "        # Check if we're done\n",
    "        quota_remaining = body['quota_remaining']\n",
    "        quota_max = body['quota_max']\n",
    "        has_more: bool = body['has_more']\n",
    "        done = not body['has_more'] or body['quota_remaining'] <= 0\n",
    "\n",
    "        print(f'Got {pagesize} questions from page #{page} (quota: {quota_remaining}/{quota_max})')\n",
    "\n",
    "\n",
    "        # Check if we need to back off before sending more requests. Only necessary if we're not done.\n",
    "        backoff = body.get('backoff', 0)\n",
    "        if not done and backoff > 0:\n",
    "            print(f'Backoff requested, sleeping for {backoff} seconds')\n",
    "            time.sleep(backoff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping questions\n",
      "Fetching page 1\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "400 Bad Request API returned error 502: too many requests from this IP, more requests available in 12717 seconds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1279/1153942043.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Scraping questions'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Scrape each page, bulk inserting each one into mongo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_stackoverflow_questions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxpages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpagesize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpage_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1279/4154703920.py\u001b[0m in \u001b[0;36mget_stackoverflow_questions\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0merror_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{r.status_code} {r.reason} API returned error {error_json[\"error_id\"]}: {error_json[\"error_message\"]}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;34m'json'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content-type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# We're expecting JSON back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 400 Bad Request API returned error 502: too many requests from this IP, more requests available in 12717 seconds"
     ]
    }
   ],
   "source": [
    "# This takes a while, is expensive, and is only necessary once. This flag\n",
    "# lets you skip this step if you've already run it.\n",
    "should_scrape = True\n",
    "drop = False\n",
    "page_size = 100 # Number of questions to return per page\n",
    "page = 1       # Starting page index, 1-indexed. Useful for continuing where you left off in the event of a crash\n",
    "\n",
    "if should_scrape:\n",
    "\n",
    "    if drop:\n",
    "        print('Dropping questions collection')\n",
    "        questions.drop()\n",
    "        \n",
    "    print('Scraping questions')\n",
    "    # Scrape each page, bulk inserting each one into mongo\n",
    "    for page in get_stackoverflow_questions(page=page, maxpages=100, pagesize=page_size):\n",
    "        if type(page) is not list:\n",
    "            assert type(page) is dict\n",
    "            page = [page]\n",
    "\n",
    "        page = filter(lambda q: q['answer_count'] > 0, page)\n",
    "        upserts = [UpdateOne({'_id': q['question_id']}, {'$set': q}, upsert=True) for q in page]\n",
    "        questions.bulk_write(upserts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping StackOverflow Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_questions(**kwargs):\n",
    "    pagesize = kwargs.get('pagesize', 100) # How many questions to return per page\n",
    "    assert 1 <= pagesize \n",
    "\n",
    "    page = kwargs.get('page', 1)           # Starting page index, 1-indexed\n",
    "    assert page >= 1\n",
    "\n",
    "    # Calculate number of documents to skip\n",
    "    skips = page_size * (page_num - 1)\n",
    "\n",
    "    # Skip and limit\n",
    "    cursor = questions.find().skip(skips).limit(page_size)\n",
    "    for doc in cursor:\n",
    "        yield doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static const double a = 2 * M_PI * 280 * 30e-6;\n",
      "static const double dx = cos(a);\n",
      "static const double dy = sin(a);\n",
      "double x = 1, y = 0; // complex x + iy\n",
      "int counter = 0;\n",
      "\n",
      "void control_loop() {\n",
      "    double xx = dx*x - dy*y;\n",
      "    double yy = dx*y + dy*x;\n",
      "    x = xx, y = yy;\n",
      "\n",
      "    // renormalize once in a while, based on\n",
      "    // https://www.gamedev.net/forums/topic.asp?topic_id=278849\n",
      "    if((counter++ & 0xff) == 0) {\n",
      "        double d = 1 - (x*x + y*y - 1)/2;\n",
      "        x *= d, y *= d;\n",
      "    }\n",
      "\n",
      "    double sine = y; // this is your sine\n",
      "}\n",
      "\n",
      "xx = cos((n+1)*a) = cos(n*a)*cos(a) - sin(n*a)*sin(a) = x*dx - y*dy\n",
      "yy = sin((n+1)*a) = sin(n*a)*cos(a) + cos(n*a)*sin(a) = y*dx + x*dy\n",
      "\n",
      "double d = 1/sqrt(x*x + y*y);\n",
      "x *= d, y *= d;\n",
      "\n",
      "d = 1 - (x*x + y*y - 1)/2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def scrape_stackoverflow_page(url: str) -> List[Dict]:\n",
    "\n",
    "    # Load the page into BeautifulSoup\n",
    "    r = session.get(url)\n",
    "    html_doc = r.text\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "    answers = soup.select('.answer')\n",
    "\n",
    "    answers_parsed = []\n",
    "    for answer in answers:\n",
    "        answer_cell = answer.select_one('.answercell')\n",
    "\n",
    "        answer_id = int(answer['data-answerid'])\n",
    "\n",
    "        # Get all code snippet elements for the answer, skipping if there are none\n",
    "        snippet_elems = answer_cell.select('pre > code')\n",
    "        if not len(snippet_elems):\n",
    "            continue\n",
    "\n",
    "        # Contains the user name and id of the answerer\n",
    "        user_details = answer.select_one('.post-signature .user-details > a')\n",
    "\n",
    "        # Extract the answer author's user id. Anonymous users have no user id\n",
    "        if user_details is None:\n",
    "            user_id = None\n",
    "            user_name = 'anonymous'\n",
    "        else:\n",
    "            _, _, user_id, user_name = user_details['href'].split('/') # takes form /users/:id/:name\n",
    "            user_id = int(user_id) # May be -1 if posted by 'community'\n",
    "\n",
    "        answer_data = {\n",
    "            # 'question_id': question_id,\n",
    "            'snippets': '\\n'.join([code_block.text for code_block in snippet_elems]),\n",
    "            'score': int(answer['data-score']),\n",
    "            'answer_id': answer_id,\n",
    "            'page_pos': int(answer['data-position-on-page']),\n",
    "            'is_highest_scored': answer['data-highest-scored'] == '1',\n",
    "            'question_has_highest_accepted_answer': answer['data-question-has-accepted-highest-score'] == '1',\n",
    "            # 'is_accepted': answer.has_class('accepted-answer'),\n",
    "            'is_accepted': 'accepted-answer' in answer['class'],\n",
    "            # 'source': answer.select_one('a.js-share-link').get('href').strip(),\n",
    "            'source': f'https://stackoverflow.com/a/{answer_id}',\n",
    "            'author_id': user_id,\n",
    "            'author_username': user_name,\n",
    "        }\n",
    "\n",
    "        answers_parsed.append(answer_data)\n",
    "\n",
    "    return answers_parsed\n",
    "\n",
    "# Test that the scraper works\n",
    "test_data = scrape_stackoverflow_page('https://stackoverflow.com/questions/69729326/endless-sine-generation-in-c')\n",
    "\n",
    "assert type(test_data) is list\n",
    "assert len(test_data) > 0\n",
    "\n",
    "for answer in test_data:\n",
    "    assert type(answer) is dict\n",
    "    # assert answer['question_id'] == 69729326 # This is the question we're scraping\n",
    "    assert 'snippets' in answer\n",
    "    assert 'score' in answer\n",
    "    assert 'answer_id' in answer\n",
    "    assert 'page_pos' in answer\n",
    "    assert 'is_highest_scored' in answer\n",
    "    assert 'question_has_highest_accepted_answer' in answer\n",
    "    assert 'is_accepted' in answer\n",
    "    assert 'source' in answer\n",
    "    assert 'author_id' in answer\n",
    "    assert 'author_username' in answer\n",
    "\n",
    "print(test_data[0]['snippets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 158/169 ....xxx...x.x..xxx.x.x..xxxxxx..xxxxx.x.x.xxx..xx.x.x.x.....x..x..xx....x..x.x.....x..xx....xxx.x..x\n",
      "Scraping page 159/169 xx....x..x...xxxx..x..........x..x..xxx...x.xxx.xx.xx.x........x...x....x..xxx.xx.......xx.x.x.....x\n",
      "Scraping page 160/169 xx.xxx."
     ]
    }
   ],
   "source": [
    "drop = False         # Set to True to drop the collection before scraping\n",
    "page_size = 100      # The number of questions to scrape in each page\n",
    "start_page = 158     # The page to start scraping at, allows for resuming scraping after a crash\n",
    "should_scrape = True # Set to True to scrape the questions collection\n",
    "num_pages = int(questions.count_documents({}) / page_size)\n",
    "\n",
    "\n",
    "if should_scrape:\n",
    "\n",
    "    # Drop the collection if we're dropping it\n",
    "    if drop:\n",
    "        print('Dropping answers collection') \n",
    "        answers.drop()\n",
    "\n",
    "    # Scrape each page of questions, bulk inserting answers into mongo\n",
    "    assert start_page > 0\n",
    "    for page_num in range(start_page, num_pages + 1):\n",
    "        # page_num = i + 1\n",
    "        print(f'Scraping page {page_num}/{num_pages} ', end = '')\n",
    "\n",
    "        for question in get_questions(page=page_num, pagesize=page_size):\n",
    "\n",
    "            # Get the answers for this question, skipping if no relevant answers are available\n",
    "            answers_data = scrape_stackoverflow_page(question['link'])\n",
    "            if not len(answers_data):\n",
    "                print('x', end = '')\n",
    "                continue\n",
    "\n",
    "            # Add the question id to each answer\n",
    "            for answer_data in answers_data:\n",
    "                answer_data['question_id'] = question['question_id']\n",
    "\n",
    "            # Bulk insert the answers\n",
    "            upserts = [UpdateOne({'_id': answer['answer_id']}, {'$set': answer}, upsert=True) for answer in answers_data]\n",
    "            answers.bulk_write(upserts)\n",
    "            print('.', end = '')\n",
    "            time.sleep(0.60 + random.random()) # Don't spam the server, otherwise CloudFlare will complain\n",
    "\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
